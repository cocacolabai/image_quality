{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# read path\n",
    "import os \n",
    "\n",
    "# image \n",
    "import cv2\n",
    "\n",
    "# time \n",
    "import datetime as dt\n",
    "\n",
    "# urlib\n",
    "import urllib\n",
    "\n",
    "# Paralel Processing\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# clean garbage\n",
    "import gc\n",
    "\n",
    "# modeling sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "\n",
    "bad_images_path = os.path.join(current_path, 'bad_images')\n",
    "good_images_path = os.path.join(current_path, 'good_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_image_list = os.listdir(bad_images_path)\n",
    "bad_image_list = [os.path.join(bad_images_path, image_file) for image_file in bad_image_list]\n",
    "\n",
    "good_image_list = os.listdir(good_images_path)\n",
    "good_image_list = [os.path.join(good_images_path, image_file) for image_file in good_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 20 \n",
    "img_height = 30\n",
    "img_channel = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_path):\n",
    "    try:\n",
    "        req = urllib.urlopen(img_path)\n",
    "        arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "        img = cv2.imdecode(arr,-1) # 'load it as it is'\n",
    "\n",
    "        if img.shape[1] > img.shape[0]: #rotate if landscape\n",
    "            img = np.rot90(img)\n",
    "        img = cv2.resize(img, (img_width, img_height)) # resize with width = 200 , height = 300  \n",
    "\n",
    "        return np.reshape(img, (-1,img_height, img_width, img_channel))\n",
    "    except:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_image_array = []\n",
    "good_image_array = []\n",
    "\n",
    "#read image\n",
    "for img_path in bad_image_list:\n",
    "    img_array = get_img_array(img_path)\n",
    "    if type(img_array) == np.ndarray:\n",
    "#         bad_image_array = np.append(bad_image_array, img_array, axis = 0)\n",
    "        bad_image_array.append(img_array)\n",
    "\n",
    "\n",
    "for img_path in good_image_list:\n",
    "    img_array = get_img_array(img_path)\n",
    "    if type(img_array) == np.ndarray:\n",
    "#         good_image_array = np.append(good_image_array, img_array, axis = 0)\n",
    "        good_image_array.append(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bad_image_list, good_image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_image_data = np.empty((len(bad_image_array),img_height,img_width, img_channel), dtype = int)\n",
    "\n",
    "for ix in range(len(bad_image_array)): \n",
    "    bad_image_data[ix,:,:,:] = bad_image_array[ix][0]\n",
    "    \n",
    "good_image_data = np.empty((len(good_image_array),img_height,img_width, img_channel), dtype = int)\n",
    "\n",
    "for ix in range(len(good_image_array)): \n",
    "    good_image_data[ix,:,:,:] = good_image_array[ix][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bad_image_array, good_image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.append(bad_image_data, good_image_data, axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_image_label = [0] * len(bad_image_data)\n",
    "good_image_label = [1] * len(good_image_data)\n",
    "\n",
    "all_label = bad_image_label + good_image_label\n",
    "\n",
    "# transform one hot encoding\n",
    "image_label = np.zeros([len(all_label), len(np.unique(all_label))])\n",
    "for i in range(image_label.shape[0]):\n",
    "    image_label[i,all_label[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(image_data, image_label, stratify = image_label ,test_size = 0.1, random_state = 210)\n",
    "# X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Parameter\n",
    "\n",
    "image_height = img_height #30\n",
    "image_width = img_width #20\n",
    "num_channels = img_channel #3\n",
    "\n",
    "num_targets = 2\n",
    "generations = 2000\n",
    "\n",
    "layer_hidden_1 = 384\n",
    "layer_hidden_2 = 192\n",
    "\n",
    "eval_every = 500\n",
    "batch_size = 1024\n",
    "\n",
    "# Exponential Learning Rate Decay Params\n",
    "learning_rate = 0.1\n",
    "lr_decay = 0.1\n",
    "num_gens_to_wait = 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters of convolutional layer\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "#parameters of pooling layer\n",
    "pool2_fmaps = conv2_fmaps\n",
    "#parameters of fully connected network and outputs\n",
    "n_fc1 = 64\n",
    "n_outputs = num_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, img_height,img_width, img_channel], name = \"X\")\n",
    "    y = tf.placeholder(tf.int32, shape = [None, num_targets], name = \"y\")\n",
    "    \n",
    "with tf.name_scope(\"conv1\"):\n",
    "    conv1 = tf.layers.conv2d(X, filters=conv1_fmaps, kernel_size = conv1_ksize,\n",
    "                         strides = conv1_stride, padding=conv1_pad,\n",
    "                         activation = tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "with tf.name_scope(\"conv2\"):\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"pool2\"):\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool2_flat = tf.reshape(pool2, shape=[-1,pool2_fmaps*7*7])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool2_flat, n_fc1, activation = tf.nn.relu, name = \"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name = \"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "#     # Make sure targets are integers and drop extra dimensions\n",
    "#     targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "#     # Get predicted values by finding which logit is the greatest\n",
    "#     batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "#     # Check if they are equal across the batch\n",
    "#     predicted_correctly = tf.equal(batch_predictions, targets)\n",
    "#     # Average the 1's and 0's (True's and False's) across the batch size\n",
    "#     accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32))\n",
    "#     return(accuracy)\n",
    "    \n",
    "    y_squeeze = tf.squeeze(tf.cast(y, tf.int32))\n",
    "    prediction = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    \n",
    "    \n",
    "    correct = tf.equal(prediction, y_squeeze)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    \n",
    "#     correct = tf.nn.in_top_k(logits,y,1)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "batch_size = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train) // batch_size):\n",
    "            #this cycle is for dividing step by step the heavy work of each neuron\n",
    "            X_batch = X_train[iteration*batch_size:iteration*batch_size+batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:iteration*batch_size+batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "#         acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "#         acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "#         print(\"Epoch:\",epoch+1, \"Train accuracy:\", acc_train, \"test accuracy:\", acc_test)\n",
    "       \n",
    "        save_path = saver.save(sess, \"./my_CNN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Parameter\n",
    "\n",
    "image_height = img_height #30\n",
    "image_width = img_width #20\n",
    "num_channels = img_channel #3\n",
    "\n",
    "num_targets = 2\n",
    "generations = 2000\n",
    "\n",
    "layer_hidden_1 = 384\n",
    "layer_hidden_2 = 192\n",
    "\n",
    "eval_every = 500\n",
    "batch_size = 1024\n",
    "\n",
    "# Exponential Learning Rate Decay Params\n",
    "learning_rate = 0.1\n",
    "lr_decay = 0.1\n",
    "num_gens_to_wait = 25.\n",
    "\n",
    "\n",
    "# Define the model architecture, this will return logits from images\n",
    "def cnn_model(input_images, batch_size, train_logical=True):\n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return(tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.truncated_normal_initializer(stddev=0.05)))\n",
    "    def zero_var(name, shape, dtype):\n",
    "        return(tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.constant_initializer(0.0)))\n",
    "    \n",
    "    # First Convolutional Layer\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        # Conv_kernel is 5x5 for all 3 colors and we will create 64 features\n",
    "        conv1_kernel = truncated_normal_var(name='conv_kernel1', shape=[5, 5, 3, 64], dtype=tf.float32)\n",
    "        # We convolve across the image with a stride size of 1\n",
    "        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        # Initialize and add the bias term\n",
    "        conv1_bias = zero_var(name='conv_bias1', shape=[64], dtype=tf.float32)\n",
    "        conv1_add_bias = tf.nn.bias_add(conv1, conv1_bias)\n",
    "        # ReLU element wise\n",
    "        relu_conv1 = tf.nn.relu(conv1_add_bias)\n",
    "    \n",
    "    # Max Pooling\n",
    "    pool1 = tf.nn.max_pool(relu_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],padding='SAME', name='pool_layer1')\n",
    "    \n",
    "    # Local Response Normalization (parameters from paper)\n",
    "    # paper: http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
    "    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm1')\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        # Conv kernel is 5x5, across all prior 64 features and we create 64 more features\n",
    "        conv2_kernel = truncated_normal_var(name='conv_kernel2', shape=[5, 5, 64, 64], dtype=tf.float32)\n",
    "        # Convolve filter across prior output with stride size of 1\n",
    "        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        # Initialize and add the bias\n",
    "        conv2_bias = zero_var(name='conv_bias2', shape=[64], dtype=tf.float32)\n",
    "        conv2_add_bias = tf.nn.bias_add(conv2, conv2_bias)\n",
    "        # ReLU element wise\n",
    "        relu_conv2 = tf.nn.relu(conv2_add_bias)\n",
    "    \n",
    "    # Max Pooling\n",
    "    pool2 = tf.nn.max_pool(relu_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_layer2')    \n",
    "    \n",
    "     # Local Response Normalization (parameters from paper)\n",
    "    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm2')\n",
    "    \n",
    "    # Reshape output into a single matrix for multiplication for the fully connected layers\n",
    "    reshaped_output = tf.reshape(norm2, [batch_size, -1])\n",
    "    reshaped_dim = reshaped_output.get_shape()[1].value\n",
    "    \n",
    "\n",
    "    \n",
    "    # First Fully Connected Layer\n",
    "    with tf.variable_scope('full1') as scope:\n",
    "        # Fully connected layer will have 384 outputs.\n",
    "        full_weight1 = truncated_normal_var(name='full_mult1', shape=[reshaped_dim, layer_hidden_1], dtype=tf.float32)\n",
    "        full_bias1 = zero_var(name='full_bias1', shape=[384], dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, full_weight1), full_bias1))\n",
    "\n",
    "    # Second Fully Connected Layer\n",
    "    with tf.variable_scope('full2') as scope:\n",
    "        # Second fully connected layer has 192 outputs.\n",
    "        full_weight2 = truncated_normal_var(name='full_mult2', shape=[layer_hidden_1, layer_hidden_2], dtype=tf.float32)\n",
    "        full_bias2 = zero_var(name='full_bias2', shape=[192], dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, full_weight2), full_bias2))\n",
    "\n",
    "    # Final Fully Connected Layer -> 10 categories for output (num_targets)\n",
    "    with tf.variable_scope('full3') as scope:\n",
    "        # Final fully connected layer has 10 (num_targets) outputs.\n",
    "        full_weight3 = truncated_normal_var(name='full_mult3', shape=[layer_hidden_2, num_targets], dtype=tf.float32)\n",
    "        full_bias3 =  zero_var(name='full_bias3', shape=[num_targets], dtype=tf.float32)\n",
    "        final_output = tf.add(tf.matmul(full_layer2, full_weight3), full_bias3)\n",
    "        \n",
    "    return(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def model_loss(logits, targets):\n",
    "    # Get rid of extra dimensions and cast targets into integers\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    # Calculate cross entropy from logits and targets\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, targets)\n",
    "    # Take the average loss across batch size\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    return(cross_entropy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train step\n",
    "def train_step(loss_value, generation_num):\n",
    "    # Our learning rate is an exponential decay after we wait a fair number of generations\n",
    "    model_learning_rate = tf.train.exponential_decay(learning_rate, generation_num,\n",
    "                                                     num_gens_to_wait, lr_decay, staircase=True)\n",
    "    # Create optimizer\n",
    "    my_optimizer = tf.train.GradientDescentOptimizer(model_learning_rate)\n",
    "    # Initialize train step\n",
    "    train_step = my_optimizer.minimize(loss_value)\n",
    "    return(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def accuracy_of_batch(logits, targets):\n",
    "    # Make sure targets are integers and drop extra dimensions\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    # Get predicted values by finding which logit is the greatest\n",
    "    batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    # Check if they are equal across the batch\n",
    "    predicted_correctly = tf.equal(batch_predictions, targets)\n",
    "    # Average the 1's and 0's (True's and False's) across the batch size\n",
    "    accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32))\n",
    "    return(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Model\n",
    "print('Creating Model.')\n",
    "with tf.variable_scope('model_definition') as scope:\n",
    "    # Declare the training network model\n",
    "    model_output = cnn_model(X_train, batch_size)\n",
    "    # This is very important!!!  We must set the scope to REUSE the variables,\n",
    "    #  otherwise, when we set the test network model, it will create new random\n",
    "    #  variables.  Otherwise we get random evaluations on the test batches.\n",
    "    scope.reuse_variables()\n",
    "    test_output = cifar_cnn_model(test_images, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Variable Functions (weights and bias)\n",
    "def init_weight(shape, st_dev):\n",
    "    weight = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n",
    "    return(weight)\n",
    "    \n",
    "\n",
    "def init_bias(shape, st_dev):\n",
    "    bias = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n",
    "    return(bias)\n",
    "\n",
    "\n",
    "# Create a fully connected layer:\n",
    "def fully_connected(input_layer, weights, biases):\n",
    "    layer = tf.add(tf.matmul(input_layer, weights), biases)\n",
    "    return(tf.nn.relu(layer))\n",
    "\n",
    "def fully_connected_layer(input_layer, layer_input, layer_output, st_dev=10.0):\n",
    "    weight = init_weight(shape=[layer_input, layer_output], st_dev=st_dev)\n",
    "    bias = init_bias(shape=[layer_output], st_dev=st_dev)\n",
    "    layer = fully_connected(input_layer, weight, bias)\n",
    "    return(layer)\n",
    "\n",
    "def last_main_layer(input_layer, layer_input, layer_output, st_dev=10.0):\n",
    "    weight = init_weight(shape=[layer_input, layer_output], st_dev=st_dev)\n",
    "    bias = init_bias(shape=[layer_output], st_dev=st_dev)\n",
    "    layer = tf.add(tf.matmul(input_layer, weight), bias)\n",
    "    return(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph session \n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameter\n",
    "n_class = 2\n",
    "\n",
    "n_features = 90\n",
    "n_hidden1 = 250\n",
    "n_hidden2 = 1000\n",
    "n_hidden3 = 400\n",
    "n_hidden4 = 350\n",
    "\n",
    "dropout_rate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Placeholders\n",
    "x_data = tf.placeholder(shape=[None, n_features], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, n_class], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = fully_connected_layer(x_data, n_features, n_hidden1)\n",
    "layer_1 = tf.layers.dropout(layer_1, rate=dropout_rate)\n",
    "\n",
    "layer_2 = fully_connected_layer(layer_1, n_hidden1, n_hidden2)\n",
    "layer_2 = tf.layers.dropout(layer_2, rate=dropout_rate)\n",
    "\n",
    "layer_3 = fully_connected_layer(layer_2, n_hidden2, n_hidden3)\n",
    "layer_3 = tf.layers.dropout(layer_3, rate=dropout_rate)\n",
    "\n",
    "layer_4 = fully_connected_layer(layer_3, n_hidden3, n_hidden4)\n",
    "layer_4 = tf.layers.dropout(layer_4, rate=dropout_rate)\n",
    "\n",
    "layer_last = last_main_layer(layer_4, n_hidden4, n_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y_target, logits = layer_last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare optimizer\n",
    "learning_rate = 0.05\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train[rand_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "batch_size = len(X_train)\n",
    "\n",
    "loss_vec = []\n",
    "test_loss = []\n",
    "for i in range(3000):\n",
    "    rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "    rand_x = X_train[rand_index]\n",
    "    rand_y = y_train[rand_index]\n",
    "    sess.run(optimizer, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    test_temp_loss = sess.run(loss, feed_dict={x_data: X_test, y_target: y_test})\n",
    "    test_loss.append(test_temp_loss)\n",
    "    if (i+1)%50==0:\n",
    "        print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(layer_last, 1), tf.argmax(y_target, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print(\"Accuracy_train:\", sess.run(accuracy, feed_dict = {x_data: X_train, y_target: y_train}) )\n",
    "        print(\"Accuracy_test:\", sess.run(accuracy, feed_dict = {x_data: X_test, y_target: y_test}))\n",
    "        \n",
    "\n",
    "#         global result \n",
    "#         result = tf.argmax(pred, 1).eval({x: X_test, y: y_test})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
