{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# read path\n",
    "import os \n",
    "\n",
    "# image \n",
    "import cv2\n",
    "\n",
    "# time \n",
    "import datetime as dt\n",
    "\n",
    "# urlib\n",
    "import urllib\n",
    "\n",
    "# Paralel Processing\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# clean garbage\n",
    "import gc\n",
    "\n",
    "# modeling sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "\n",
    "bad_images_path = os.path.join(current_path, 'bad_images')\n",
    "good_images_path = os.path.join(current_path, 'good_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/dev/ana/Hervind/image/bad_images'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_image_list = os.listdir(bad_images_path)\n",
    "bad_image_list = [os.path.join(bad_images_path, image_file) for image_file in bad_image_list]\n",
    "\n",
    "good_image_list = os.listdir(good_images_path)\n",
    "good_image_list = [os.path.join(good_images_path, image_file) for image_file in good_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10821"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8115"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_image0 = bad_image_list[0]\n",
    "\n",
    "img = cv2.imread(bad_image0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "req = urllib.urlopen(os.path.join(bad_images_path, bad_image0))\n",
    "arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "img = cv2.imdecode(arr,-1) # 'load it as it is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init : define required function (get image basic properties) --- 2018-04-20 01:37:46.723124\n"
     ]
    }
   ],
   "source": [
    "print \"init : define required function (get image basic properties) --- \" + str(dt.datetime.now())\n",
    "\n",
    "def image_colorfulness(img):\n",
    "    (B, G, R) = cv2.split(img.astype(\"float\"))\n",
    "    rg = np.absolute(R - G)\n",
    "    yb = np.absolute(0.5 * (R + G) - B)\n",
    "    (rbMean, rbStd) = (np.mean(rg), np.std(rg))\n",
    "    (ybMean, ybStd) = (np.mean(yb), np.std(yb))\n",
    "    stdRoot = np.sqrt((rbStd ** 2) + (ybStd ** 2))\n",
    "    meanRoot = np.sqrt((rbMean ** 2) + (ybMean ** 2))\n",
    "    colorfulness = stdRoot + (0.3 * meanRoot)\n",
    "    return colorfulness\n",
    "\n",
    "def get_properties(img):\n",
    "    img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)   \n",
    "    \n",
    "    luminance = img_gray.mean()\n",
    "    \n",
    "    gy, gx = np.gradient(img_gray)\n",
    "    gnorm = gx**2 + gy**2\n",
    "    sharpness = np.average(gnorm)\n",
    "    \n",
    "    dist_pixel = len(np.unique([str(x) for x in img.reshape(-1, 3).tolist()]))\n",
    "\n",
    "    blur = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "    \n",
    "    saturation = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1].mean()\n",
    "    \n",
    "    \n",
    "    red   = img[:,:,2].mean()\n",
    "    green = img[:,:,1].mean()\n",
    "    blue  = img[:,:,0].mean()\n",
    "    \n",
    "    color = image_colorfulness(img)\n",
    "    \n",
    "    return dist_pixel, blur, sharpness, saturation, luminance, red, green, blue, color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init : define required function (get image properties) --- 2018-04-20 01:37:47.304108\n"
     ]
    }
   ],
   "source": [
    "print \"init : define required function (get image properties) --- \" + str(dt.datetime.now())\n",
    "\n",
    "def get_img_properties(img_path):\n",
    "    try: \n",
    "        \n",
    "        req = urllib.urlopen(img_path)\n",
    "        arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "        img = cv2.imdecode(arr,-1) # 'load it as it is'\n",
    "\n",
    "        if img.shape[1] > img.shape[0]: #rotate if landscape\n",
    "            img = np.rot90(img)\n",
    "        img = cv2.resize(img, (200, 300)) # resize with width = 200 , height = 300    \n",
    "\n",
    "        al = img[:100, :65, :]; ac = img[:100, 65:130, :]; ar = img[:100, 130:, :]\n",
    "        ml = img[100:200, :65, :]; mc = img[100:200, 65:130, :]; mr = img[100:200, 130:, :]\n",
    "        bl = img[200:, :65, :]; bc = img[200:, 65:130, :]; br = img[200:, 130:, :]\n",
    "\n",
    "        dist_pixel, blur, sharpness, saturation, luminance, red, green, blue, color = get_properties(img)\n",
    "\n",
    "        al_dist_pixel, al_blur, al_sharpness, al_saturation, al_luminance, al_red, al_green, al_blue, al_color = get_properties(al)\n",
    "        ac_dist_pixel, ac_blur, ac_sharpness, ac_saturation, ac_luminance, ac_red, ac_green, ac_blue, ac_color = get_properties(ac)\n",
    "        ar_dist_pixel, ar_blur, ar_sharpness, ar_saturation, ar_luminance, ar_red, ar_green, ar_blue, ar_color = get_properties(ar)\n",
    "\n",
    "        ml_dist_pixel, ml_blur, ml_sharpness, ml_saturation, ml_luminance, ml_red, ml_green, ml_blue, ml_color = get_properties(ml)\n",
    "        mc_dist_pixel, mc_blur, mc_sharpness, mc_saturation, mc_luminance, mc_red, mc_green, mc_blue, mc_color = get_properties(mc)\n",
    "        mr_dist_pixel, mr_blur, mr_sharpness, mr_saturation, mr_luminance, mr_red, mr_green, mr_blue, mr_color = get_properties(mr)\n",
    "\n",
    "        bl_dist_pixel, bl_blur, bl_sharpness, bl_saturation, bl_luminance, bl_red, bl_green, bl_blue, bl_color = get_properties(bl)\n",
    "        bc_dist_pixel, bc_blur, bc_sharpness, bc_saturation, bc_luminance, bc_red, bc_green, bc_blue, bc_color = get_properties(bc)\n",
    "        br_dist_pixel, br_blur, br_sharpness, br_saturation, br_luminance, br_red, br_green, br_blue, br_color = get_properties(br)\n",
    "\n",
    "        stg = {'local_path' : img_path, 'dist_pixel' : dist_pixel, 'blur' : blur, 'sharpness' : sharpness, 'saturation' : saturation, 'luminance' : luminance, 'red' : red, 'green' : green, 'blue' : blue, 'color' : color,\n",
    "                'al_dist_pixel' : al_dist_pixel, 'al_blur' : al_blur, 'al_sharpness' : al_sharpness, 'al_saturation' : al_saturation, 'al_luminance' : al_luminance, 'al_red' : al_red, 'al_green' : al_green, 'al_blue' : al_blue, 'al_color' : al_color,\n",
    "                'ac_dist_pixel' : ac_dist_pixel, 'ac_blur' : ac_blur, 'ac_sharpness' : ac_sharpness, 'ac_saturation' : ac_saturation, 'ac_luminance' : ac_luminance, 'ac_red' : ac_red, 'ac_green' : ac_green, 'ac_blue' : ac_blue, 'ac_color' : ac_color,\n",
    "                'ar_dist_pixel' : ar_dist_pixel, 'ar_blur' : ar_blur, 'ar_sharpness' : ar_sharpness, 'ar_saturation' : ar_saturation, 'ar_luminance' : ar_luminance, 'ar_red' : ar_red, 'ar_green' : ar_green, 'ar_blue' : ar_blue, 'ar_color' : ar_color,\n",
    "                'ml_dist_pixel' : ml_dist_pixel, 'ml_blur' : ml_blur, 'ml_sharpness' : ml_sharpness, 'ml_saturation' : ml_saturation, 'ml_luminance' : ml_luminance, 'ml_red' : ml_red, 'ml_green' : ml_green, 'ml_blue' : ml_blue, 'ml_color' : ml_color,\n",
    "                'mc_dist_pixel' : mc_dist_pixel, 'mc_blur' : mc_blur, 'mc_sharpness' : mc_sharpness, 'mc_saturation' : mc_saturation, 'mc_luminance' : mc_luminance, 'mc_red' : mc_red, 'mc_green' : mc_green, 'mc_blue' : mc_blue, 'mc_color' : mc_color,\n",
    "                'mr_dist_pixel' : mr_dist_pixel, 'mr_blur' : mr_blur, 'mr_sharpness' : mr_sharpness, 'mr_saturation' : mr_saturation, 'mr_luminance' : mr_luminance, 'mr_red' : mr_red, 'mr_green' : mr_green, 'mr_blue' : mr_blue, 'mr_color' : mr_color,\n",
    "                'bl_dist_pixel' : bl_dist_pixel, 'bl_blur' : bl_blur, 'bl_sharpness' : bl_sharpness, 'bl_saturation' : bl_saturation, 'bl_luminance' : bl_luminance, 'bl_red' : bl_red, 'bl_green' : bl_green, 'bl_blue' : bl_blue, 'bl_color' : bl_color,\n",
    "                'bc_dist_pixel' : bc_dist_pixel, 'bc_blur' : bc_blur, 'bc_sharpness' : bc_sharpness, 'bc_saturation' : bc_saturation, 'bc_luminance' : bc_luminance, 'bc_red' : bc_red, 'bc_green' : bc_green, 'bc_blue' : bc_blue, 'bc_color' : bc_color,\n",
    "                'br_dist_pixel' : br_dist_pixel, 'br_blur' : br_blur, 'br_sharpness' : br_sharpness, 'br_saturation' : br_saturation, 'br_luminance' : br_luminance, 'br_red' : br_red, 'br_green' : br_green, 'br_blue' : br_blue, 'br_color' : br_color} \n",
    "        \n",
    "        return stg\n",
    "    \n",
    "    except:        \n",
    "        # set result = null\n",
    "        stg = {'local_path' : img_path, 'dist_pixel' : None, 'blur' : None, 'sharpness' : None, 'saturation' : None, 'luminance' : None, 'red' : None, 'green' : None, 'blue' : None, 'color' : None,\n",
    "                'al_dist_pixel' : None, 'al_blur' : None, 'al_sharpness' : None, 'al_saturation' : None, 'al_luminance' : None, 'al_red' : None, 'al_green' : None, 'al_blue' : None, 'al_color' : None,\n",
    "                'ac_dist_pixel' : None, 'ac_blur' : None, 'ac_sharpness' : None, 'ac_saturation' : None, 'ac_luminance' : None, 'ac_red' : None, 'ac_green' : None, 'ac_blue' : None, 'ac_color' : None,\n",
    "                'ar_dist_pixel' : None, 'ar_blur' : None, 'ar_sharpness' : None, 'ar_saturation' : None, 'ar_luminance' : None, 'ar_red' : None, 'ar_green' : None, 'ar_blue' : None, 'ar_color' : None,\n",
    "                'ml_dist_pixel' : None, 'ml_blur' : None, 'ml_sharpness' : None, 'ml_saturation' : None, 'ml_luminance' : None, 'ml_red' : None, 'ml_green' : None, 'ml_blue' : None, 'ml_color' : None,\n",
    "                'mc_dist_pixel' : None, 'mc_blur' : None, 'mc_sharpness' : None, 'mc_saturation' : None, 'mc_luminance' : None, 'mc_red' : None, 'mc_green' : None, 'mc_blue' : None, 'mc_color' : None,\n",
    "                'mr_dist_pixel' : None, 'mr_blur' : None, 'mr_sharpness' : None, 'mr_saturation' : None, 'mr_luminance' : None, 'mr_red' : None, 'mr_green' : None, 'mr_blue' : None, 'mr_color' : None,\n",
    "                'bl_dist_pixel' : None, 'bl_blur' : None, 'bl_sharpness' : None, 'bl_saturation' : None, 'bl_luminance' : None, 'bl_red' : None, 'bl_green' : None, 'bl_blue' : None, 'bl_color' : None,\n",
    "                'bc_dist_pixel' : None, 'bc_blur' : None, 'bc_sharpness' : None, 'bc_saturation' : None, 'bc_luminance' : None, 'bc_red' : None, 'bc_green' : None, 'bc_blue' : None, 'bc_color' : None,\n",
    "                'br_dist_pixel' : None, 'br_blur' : None, 'br_sharpness' : None, 'br_saturation' : None, 'br_luminance' : None, 'br_red' : None, 'br_green' : None, 'br_blue' : None, 'br_color' : None}\n",
    "        return stg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform : get image properties --- 2018-04-20 01:37:50.006056\n",
      "finish : get image properties --- 2018-04-20 01:40:55.711644\n"
     ]
    }
   ],
   "source": [
    "##  transform : get image properties\n",
    "print \"transform : get image properties --- \" + str(dt.datetime.now())\n",
    "\n",
    "# set multi-thread process\n",
    "pool = Pool(processes=16)\n",
    "result = pool.map_async(get_img_properties, bad_image_list)\n",
    "\n",
    "# multi-thread output\n",
    "result.wait()\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "bad_image_result = []\n",
    "\n",
    "# multi-thread process to dataframe\n",
    "for row in result.get() :\n",
    "    bad_image_result.append(row)\n",
    "\n",
    "# free memory\n",
    "gc.collect()\n",
    "\n",
    "print \"finish : get image properties --- \" + str(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform : get image properties --- 2018-04-20 01:40:55.724452\n",
      "finish : get image properties --- 2018-04-20 01:45:12.598163\n"
     ]
    }
   ],
   "source": [
    "##  transform : get image properties\n",
    "print \"transform : get image properties --- \" + str(dt.datetime.now())\n",
    "\n",
    "# set multi-thread process\n",
    "pool = Pool(processes=16)\n",
    "result = pool.map_async(get_img_properties, good_image_list)\n",
    "\n",
    "# multi-thread output\n",
    "result.wait()\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "good_image_result = []\n",
    "\n",
    "# multi-thread process to dataframe\n",
    "for row in result.get() :\n",
    "    good_image_result.append(row)\n",
    "\n",
    "# free memory\n",
    "gc.collect()\n",
    "\n",
    "print \"finish : get image properties --- \" + str(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_good_image = pd.DataFrame(good_image_result)\n",
    "\n",
    "df_bad_image = pd.DataFrame(bad_image_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good_image = df_good_image.drop('local_path', axis = 1)\n",
    "df_good_image = df_good_image.loc[~df_good_image.al_blue.isnull()]\n",
    "df_good_image['quality'] = 1\n",
    "\n",
    "df_bad_image = df_bad_image.drop('local_path', axis = 1)\n",
    "df_bad_image = df_bad_image.loc[~df_bad_image.al_blue.isnull()]\n",
    "df_bad_image['quality'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = df_good_image.append(df_bad_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_data.to_csv(\"dataset_2018-04-20.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = all_data.iloc[:,:90]\n",
    "y_ = np.array(all_data['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform one hot encoding\n",
    "y = np.zeros([len(y_), len(np.unique(y_))])\n",
    "for i in range(y.shape[0]):\n",
    "    y[i,y_[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y ,test_size = 0.1, random_state = 200)\n",
    "X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17040, 90)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Variable Functions (weights and bias)\n",
    "def init_weight(shape, st_dev):\n",
    "    weight = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n",
    "    return(weight)\n",
    "    \n",
    "\n",
    "def init_bias(shape, st_dev):\n",
    "    bias = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n",
    "    return(bias)\n",
    "\n",
    "\n",
    "# Create a fully connected layer:\n",
    "def fully_connected(input_layer, weights, biases):\n",
    "    layer = tf.add(tf.matmul(input_layer, weights), biases)\n",
    "    return(tf.nn.relu(layer))\n",
    "\n",
    "def fully_connected_layer(input_layer, layer_input, layer_output, st_dev=10.0):\n",
    "    weight = init_weight(shape=[layer_input, layer_output], st_dev=st_dev)\n",
    "    bias = init_bias(shape=[layer_output], st_dev=st_dev)\n",
    "    layer = fully_connected(input_layer, weight, bias)\n",
    "    return(layer)\n",
    "\n",
    "def last_main_layer(input_layer, layer_input, layer_output, st_dev=10.0):\n",
    "    weight = init_weight(shape=[layer_input, layer_output], st_dev=st_dev)\n",
    "    bias = init_bias(shape=[layer_output], st_dev=st_dev)\n",
    "    layer = tf.add(tf.matmul(input_layer, weight), bias)\n",
    "    return(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create graph session \n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define parameter\n",
    "n_class = 2\n",
    "\n",
    "n_features = 90\n",
    "n_hidden1 = 250\n",
    "n_hidden2 = 1000\n",
    "n_hidden3 = 400\n",
    "n_hidden4 = 350\n",
    "\n",
    "dropout_rate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Placeholders\n",
    "x_data = tf.placeholder(shape=[None, n_features], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, n_class], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = fully_connected_layer(x_data, n_features, n_hidden1)\n",
    "layer_1 = tf.layers.dropout(layer_1, rate=dropout_rate)\n",
    "\n",
    "layer_2 = fully_connected_layer(layer_1, n_hidden1, n_hidden2)\n",
    "layer_2 = tf.layers.dropout(layer_2, rate=dropout_rate)\n",
    "\n",
    "layer_3 = fully_connected_layer(layer_2, n_hidden2, n_hidden3)\n",
    "layer_3 = tf.layers.dropout(layer_3, rate=dropout_rate)\n",
    "\n",
    "layer_4 = fully_connected_layer(layer_3, n_hidden3, n_hidden4)\n",
    "layer_4 = tf.layers.dropout(layer_4, rate=dropout_rate)\n",
    "\n",
    "layer_last = last_main_layer(layer_4, n_hidden4, n_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y_target, logits = layer_last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare optimizer\n",
    "learning_rate = 0.05\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17040, 2)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[rand_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 50. Loss = 3023461400000.0\n",
      "('Accuracy_train:', 0.63509387)\n",
      "('Accuracy_test:', 0.6430834)\n",
      "Generation: 100. Loss = 1246804600000.0\n",
      "('Accuracy_train:', 0.71191317)\n",
      "('Accuracy_test:', 0.687962)\n",
      "Generation: 150. Loss = 2683141000000.0\n",
      "('Accuracy_train:', 0.62658453)\n",
      "('Accuracy_test:', 0.6013728)\n",
      "Generation: 200. Loss = 2474638800000.0\n",
      "('Accuracy_train:', 0.6903169)\n",
      "('Accuracy_test:', 0.6626188)\n",
      "Generation: 250. Loss = 1347917600000.0\n",
      "('Accuracy_train:', 0.72652584)\n",
      "('Accuracy_test:', 0.6921859)\n",
      "Generation: 300. Loss = 1258687800000.0\n",
      "('Accuracy_train:', 0.73738265)\n",
      "('Accuracy_test:', 0.69482577)\n",
      "Generation: 350. Loss = 642652600000.0\n",
      "('Accuracy_train:', 0.76819247)\n",
      "('Accuracy_test:', 0.71119326)\n",
      "Generation: 400. Loss = 1019092900000.0\n",
      "('Accuracy_train:', 0.7640845)\n",
      "('Accuracy_test:', 0.71752906)\n",
      "Generation: 450. Loss = 1589245900000.0\n",
      "('Accuracy_train:', 0.70651406)\n",
      "('Accuracy_test:', 0.66420275)\n",
      "Generation: 500. Loss = 1728258100000.0\n",
      "('Accuracy_train:', 0.63233566)\n",
      "('Accuracy_test:', 0.5992608)\n",
      "Generation: 550. Loss = 506128100000.0\n",
      "('Accuracy_train:', 0.8048709)\n",
      "('Accuracy_test:', 0.7291447)\n",
      "Generation: 600. Loss = 357876900000.0\n",
      "('Accuracy_train:', 0.83215964)\n",
      "('Accuracy_test:', 0.7412883)\n",
      "Generation: 650. Loss = 361845600000.0\n",
      "('Accuracy_train:', 0.8112089)\n",
      "('Accuracy_test:', 0.72650474)\n",
      "Generation: 700. Loss = 320922650000.0\n",
      "('Accuracy_train:', 0.8369718)\n",
      "('Accuracy_test:', 0.74920803)\n",
      "Generation: 750. Loss = 631577000000.0\n",
      "('Accuracy_train:', 0.78433096)\n",
      "('Accuracy_test:', 0.7127772)\n",
      "Generation: 800. Loss = 1401711400000.0\n",
      "('Accuracy_train:', 0.692723)\n",
      "('Accuracy_test:', 0.64572334)\n",
      "Generation: 850. Loss = 248520790000.0\n",
      "('Accuracy_train:', 0.8559272)\n",
      "('Accuracy_test:', 0.74868006)\n",
      "Generation: 900. Loss = 493161050000.0\n",
      "('Accuracy_train:', 0.80152583)\n",
      "('Accuracy_test:', 0.7154171)\n",
      "Generation: 950. Loss = 191903740000.0\n",
      "('Accuracy_train:', 0.8737676)\n",
      "('Accuracy_test:', 0.77138335)\n",
      "Generation: 1000. Loss = 301098630000.0\n",
      "('Accuracy_train:', 0.84137326)\n",
      "('Accuracy_test:', 0.7518479)\n",
      "Generation: 1050. Loss = 369309580000.0\n",
      "('Accuracy_train:', 0.8189554)\n",
      "('Accuracy_test:', 0.719113)\n",
      "Generation: 1100. Loss = 152372560000.0\n",
      "('Accuracy_train:', 0.8932512)\n",
      "('Accuracy_test:', 0.7729673)\n",
      "Generation: 1150. Loss = 508833960000.0\n",
      "('Accuracy_train:', 0.7781103)\n",
      "('Accuracy_test:', 0.6958817)\n",
      "Generation: 1200. Loss = 414501540000.0\n",
      "('Accuracy_train:', 0.79137325)\n",
      "('Accuracy_test:', 0.7069694)\n",
      "Generation: 1250. Loss = 369557830000.0\n",
      "('Accuracy_train:', 0.8318075)\n",
      "('Accuracy_test:', 0.7455121)\n",
      "Generation: 1300. Loss = 242177920000.0\n",
      "('Accuracy_train:', 0.85176057)\n",
      "('Accuracy_test:', 0.7418163)\n",
      "Generation: 1350. Loss = 370349740000.0\n",
      "('Accuracy_train:', 0.7906103)\n",
      "('Accuracy_test:', 0.69746566)\n",
      "Generation: 1400. Loss = 169714630000.0\n",
      "('Accuracy_train:', 0.8705399)\n",
      "('Accuracy_test:', 0.7481521)\n",
      "Generation: 1450. Loss = 287177180000.0\n",
      "('Accuracy_train:', 0.82899064)\n",
      "('Accuracy_test:', 0.7254488)\n",
      "Generation: 1500. Loss = 162188400000.0\n",
      "('Accuracy_train:', 0.88450706)\n",
      "('Accuracy_test:', 0.7618796)\n",
      "Generation: 1550. Loss = 98160780000.0\n",
      "('Accuracy_train:', 0.9194249)\n",
      "('Accuracy_test:', 0.780887)\n",
      "Generation: 1600. Loss = 104740430000.0\n",
      "('Accuracy_train:', 0.9094484)\n",
      "('Accuracy_test:', 0.77613515)\n",
      "Generation: 1650. Loss = 405209480000.0\n",
      "('Accuracy_train:', 0.8033451)\n",
      "('Accuracy_test:', 0.7085533)\n",
      "Generation: 1700. Loss = 193392720000.0\n",
      "('Accuracy_train:', 0.8572183)\n",
      "('Accuracy_test:', 0.73600847)\n",
      "Generation: 1750. Loss = 74660550000.0\n",
      "('Accuracy_train:', 0.92740613)\n",
      "('Accuracy_test:', 0.7750792)\n",
      "Generation: 1800. Loss = 96207870000.0\n",
      "('Accuracy_train:', 0.90874416)\n",
      "('Accuracy_test:', 0.7877508)\n",
      "Generation: 1850. Loss = 155489750000.0\n",
      "('Accuracy_train:', 0.8798122)\n",
      "('Accuracy_test:', 0.75923973)\n",
      "Generation: 1900. Loss = 172249550000.0\n",
      "('Accuracy_train:', 0.8943662)\n",
      "('Accuracy_test:', 0.7523759)\n",
      "Generation: 1950. Loss = 60343947000.0\n",
      "('Accuracy_train:', 0.9314554)\n",
      "('Accuracy_test:', 0.7845829)\n",
      "Generation: 2000. Loss = 556103500000.0\n",
      "('Accuracy_train:', 0.7778756)\n",
      "('Accuracy_test:', 0.69324183)\n",
      "Generation: 2050. Loss = 58482900000.0\n",
      "('Accuracy_train:', 0.9329812)\n",
      "('Accuracy_test:', 0.77613515)\n",
      "Generation: 2100. Loss = 45203840000.0\n",
      "('Accuracy_train:', 0.9504695)\n",
      "('Accuracy_test:', 0.7845829)\n",
      "Generation: 2150. Loss = 69540676000.0\n",
      "('Accuracy_train:', 0.92447186)\n",
      "('Accuracy_test:', 0.7829989)\n",
      "Generation: 2200. Loss = 177998510000.0\n",
      "('Accuracy_train:', 0.86555165)\n",
      "('Accuracy_test:', 0.75079197)\n",
      "Generation: 2250. Loss = 25420147000.0\n",
      "('Accuracy_train:', 0.9615024)\n",
      "('Accuracy_test:', 0.7977825)\n",
      "Generation: 2300. Loss = 1012296900000.0\n",
      "('Accuracy_train:', 0.694953)\n",
      "('Accuracy_test:', 0.6425554)\n",
      "Generation: 2350. Loss = 28318566000.0\n",
      "('Accuracy_train:', 0.9600939)\n",
      "('Accuracy_test:', 0.7935586)\n",
      "Generation: 2400. Loss = 57348768000.0\n",
      "('Accuracy_train:', 0.9290493)\n",
      "('Accuracy_test:', 0.780887)\n",
      "Generation: 2450. Loss = 17136550000.0\n",
      "('Accuracy_train:', 0.97247654)\n",
      "('Accuracy_test:', 0.79883844)\n",
      "Generation: 2500. Loss = 309989050000.0\n",
      "('Accuracy_train:', 0.84407276)\n",
      "('Accuracy_test:', 0.7344245)\n",
      "Generation: 2550. Loss = 22145122000.0\n",
      "('Accuracy_train:', 0.963615)\n",
      "('Accuracy_test:', 0.80464625)\n",
      "Generation: 2600. Loss = 45710283000.0\n",
      "('Accuracy_train:', 0.9451291)\n",
      "('Accuracy_test:', 0.78194296)\n",
      "Generation: 2650. Loss = 13800507000.0\n",
      "('Accuracy_train:', 0.97153753)\n",
      "('Accuracy_test:', 0.8062302)\n",
      "Generation: 2700. Loss = 15106233000.0\n",
      "('Accuracy_train:', 0.96801645)\n",
      "('Accuracy_test:', 0.79883844)\n",
      "Generation: 2750. Loss = 56918660000.0\n",
      "('Accuracy_train:', 0.93890846)\n",
      "('Accuracy_test:', 0.7940866)\n",
      "Generation: 2800. Loss = 16369478000.0\n",
      "('Accuracy_train:', 0.97012913)\n",
      "('Accuracy_test:', 0.79831046)\n",
      "Generation: 2850. Loss = 10375939000.0\n",
      "('Accuracy_train:', 0.98292255)\n",
      "('Accuracy_test:', 0.812038)\n",
      "Generation: 2900. Loss = 66163757000.0\n",
      "('Accuracy_train:', 0.91995305)\n",
      "('Accuracy_test:', 0.7729673)\n",
      "Generation: 2950. Loss = 6473451000.0\n",
      "('Accuracy_train:', 0.98438966)\n",
      "('Accuracy_test:', 0.8099261)\n",
      "Generation: 3000. Loss = 14057873000.0\n",
      "('Accuracy_train:', 0.97036386)\n",
      "('Accuracy_test:', 0.7993664)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = len(X_train)\n",
    "\n",
    "loss_vec = []\n",
    "test_loss = []\n",
    "for i in range(3000):\n",
    "    rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "    rand_x = X_train[rand_index]\n",
    "    rand_y = y_train[rand_index]\n",
    "    sess.run(optimizer, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    test_temp_loss = sess.run(loss, feed_dict={x_data: X_test, y_target: y_test})\n",
    "    test_loss.append(test_temp_loss)\n",
    "    if (i+1)%50==0:\n",
    "        print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(layer_last, 1), tf.argmax(y_target, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print(\"Accuracy_train:\", sess.run(accuracy, feed_dict = {x_data: X_train, y_target: y_train}) )\n",
    "        print(\"Accuracy_test:\", sess.run(accuracy, feed_dict = {x_data: X_test, y_target: y_test}))\n",
    "        \n",
    "\n",
    "#         global result \n",
    "#         result = tf.argmax(pred, 1).eval({x: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 50. Loss = 869178740000.0\n",
      "('Accuracy_train:', 0.73926055)\n",
      "('Accuracy_test:', 0.6636748)\n",
      "Generation: 100. Loss = 118713340000.0\n",
      "('Accuracy_train:', 0.8921948)\n",
      "('Accuracy_test:', 0.77138335)\n",
      "Generation: 150. Loss = 7933039000.0\n",
      "('Accuracy_train:', 0.9827465)\n",
      "('Accuracy_test:', 0.80517423)\n",
      "Generation: 200. Loss = 4989391400.0\n",
      "('Accuracy_train:', 0.9870892)\n",
      "('Accuracy_test:', 0.80728614)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = len(X_train)\n",
    "\n",
    "# loss_vec = []\n",
    "# test_loss = []\n",
    "for i in range(200):\n",
    "    rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "    rand_x = X_train[rand_index]\n",
    "    rand_y = y_train[rand_index]\n",
    "    sess.run(optimizer, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    test_temp_loss = sess.run(loss, feed_dict={x_data: X_test, y_target: y_test})\n",
    "    test_loss.append(test_temp_loss)\n",
    "    if (i+1)%50==0:\n",
    "        print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(layer_last, 1), tf.argmax(y_target, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print(\"Accuracy_train:\", sess.run(accuracy, feed_dict = {x_data: X_train, y_target: y_train}) )\n",
    "        print(\"Accuracy_test:\", sess.run(accuracy, feed_dict = {x_data: X_test, y_target: y_test}))\n",
    "        \n",
    "#         global result \n",
    "#         result = tf.argmax(pred, 1).eval({x: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 50. Loss = 16004342000.0\n",
      "('Accuracy_train:', 0.9627934)\n",
      "('Accuracy_test:', 0.7893347)\n",
      "Generation: 100. Loss = 7907058000.0\n",
      "('Accuracy_train:', 0.9870892)\n",
      "('Accuracy_test:', 0.81151)\n",
      "Generation: 150. Loss = 6385039400.0\n",
      "('Accuracy_train:', 0.983216)\n",
      "('Accuracy_test:', 0.81467795)\n",
      "Generation: 200. Loss = 2837525800.0\n",
      "('Accuracy_train:', 0.992547)\n",
      "('Accuracy_test:', 0.812566)\n",
      "Generation: 250. Loss = 2265617000.0\n",
      "('Accuracy_train:', 0.9907277)\n",
      "('Accuracy_test:', 0.8093981)\n",
      "Generation: 300. Loss = 88756680000.0\n",
      "('Accuracy_train:', 0.9004695)\n",
      "('Accuracy_test:', 0.7560718)\n",
      "Generation: 350. Loss = 154070930000.0\n",
      "('Accuracy_train:', 0.882277)\n",
      "('Accuracy_test:', 0.7534319)\n",
      "Generation: 400. Loss = 5905754000.0\n",
      "('Accuracy_train:', 0.98503524)\n",
      "('Accuracy_test:', 0.8162619)\n",
      "Generation: 450. Loss = 5106475000.0\n",
      "('Accuracy_train:', 0.98309857)\n",
      "('Accuracy_test:', 0.80464625)\n",
      "Generation: 500. Loss = 7702111700.0\n",
      "('Accuracy_train:', 0.9741197)\n",
      "('Accuracy_test:', 0.8057022)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = len(X_train)\n",
    "\n",
    "# loss_vec = []\n",
    "# test_loss = []\n",
    "for i in range(500):\n",
    "    rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "    rand_x = X_train[rand_index]\n",
    "    rand_y = y_train[rand_index]\n",
    "    sess.run(optimizer, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    test_temp_loss = sess.run(loss, feed_dict={x_data: X_test, y_target: y_test})\n",
    "    test_loss.append(test_temp_loss)\n",
    "    if (i+1)%50==0:\n",
    "        print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(layer_last, 1), tf.argmax(y_target, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print(\"Accuracy_train:\", sess.run(accuracy, feed_dict = {x_data: X_train, y_target: y_train}) )\n",
    "        print(\"Accuracy_test:\", sess.run(accuracy, feed_dict = {x_data: X_test, y_target: y_test}))\n",
    "        \n",
    "#         global result \n",
    "#         result = tf.argmax(pred, 1).eval({x: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 50. Loss = 1700377700.0\n",
      "('Accuracy_train:', 0.995716)\n",
      "('Accuracy_test:', 0.81467795)\n",
      "Generation: 100. Loss = 655152800.0\n",
      "('Accuracy_train:', 0.99606806)\n",
      "('Accuracy_test:', 0.81415)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = len(X_train)\n",
    "\n",
    "# loss_vec = []\n",
    "# test_loss = []\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "    rand_x = X_train[rand_index]\n",
    "    rand_y = y_train[rand_index]\n",
    "    sess.run(optimizer, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    test_temp_loss = sess.run(loss, feed_dict={x_data: X_test, y_target: y_test})\n",
    "    test_loss.append(test_temp_loss)\n",
    "    if (i+1)%50==0:\n",
    "        print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(layer_last, 1), tf.argmax(y_target, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print(\"Accuracy_train:\", sess.run(accuracy, feed_dict = {x_data: X_train, y_target: y_train}) )\n",
    "        print(\"Accuracy_test:\", sess.run(accuracy, feed_dict = {x_data: X_test, y_target: y_test}))\n",
    "        \n",
    "#         global result \n",
    "#         result = tf.argmax(pred, 1).eval({x: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 50. Loss = 1387722400.0\n",
      "('Accuracy_train:', 0.9963615)\n",
      "('Accuracy_test:', 0.81467795)\n",
      "Generation: 100. Loss = 393528060.0\n",
      "('Accuracy_train:', 0.9983568)\n",
      "('Accuracy_test:', 0.8162619)\n",
      "Generation: 150. Loss = 401698700.0\n",
      "('Accuracy_train:', 0.9980047)\n",
      "('Accuracy_test:', 0.81362194)\n",
      "Generation: 200. Loss = 110079800.0\n",
      "('Accuracy_train:', 0.9980047)\n",
      "('Accuracy_test:', 0.8162619)\n",
      "Generation: 250. Loss = 4775484000.0\n",
      "('Accuracy_train:', 0.9744718)\n",
      "('Accuracy_test:', 0.80464625)\n",
      "Generation: 300. Loss = 310169370000.0\n",
      "('Accuracy_train:', 0.829284)\n",
      "('Accuracy_test:', 0.72439283)\n",
      "Generation: 350. Loss = 11921522000.0\n",
      "('Accuracy_train:', 0.9725939)\n",
      "('Accuracy_test:', 0.8035903)\n",
      "Generation: 400. Loss = 1869782700.0\n",
      "('Accuracy_train:', 0.9897887)\n",
      "('Accuracy_test:', 0.81151)\n",
      "Generation: 450. Loss = 1802762900.0\n",
      "('Accuracy_train:', 0.9919014)\n",
      "('Accuracy_test:', 0.8157339)\n",
      "Generation: 500. Loss = 1595267500.0\n",
      "('Accuracy_train:', 0.98838025)\n",
      "('Accuracy_test:', 0.80517423)\n",
      "Generation: 550. Loss = 310277220.0\n",
      "('Accuracy_train:', 0.997946)\n",
      "('Accuracy_test:', 0.8162619)\n",
      "Generation: 600. Loss = 2751161000.0\n",
      "('Accuracy_train:', 0.9823357)\n",
      "('Accuracy_test:', 0.7998944)\n",
      "Generation: 650. Loss = 482636500.0\n",
      "('Accuracy_train:', 0.997946)\n",
      "('Accuracy_test:', 0.8178458)\n",
      "Generation: 700. Loss = 99136460.0\n",
      "('Accuracy_train:', 0.998885)\n",
      "('Accuracy_test:', 0.81942976)\n",
      "Generation: 750. Loss = 92554060.0\n",
      "('Accuracy_train:', 0.99812204)\n",
      "('Accuracy_test:', 0.81731784)\n",
      "Generation: 800. Loss = 8209907.0\n",
      "('Accuracy_train:', 0.99941313)\n",
      "('Accuracy_test:', 0.8204858)\n",
      "Generation: 850. Loss = 23852396.0\n",
      "('Accuracy_train:', 0.99941313)\n",
      "('Accuracy_test:', 0.81942976)\n",
      "Generation: 900. Loss = 2287175.8\n",
      "('Accuracy_train:', 0.9995305)\n",
      "('Accuracy_test:', 0.8204858)\n",
      "Generation: 950. Loss = 69122200.0\n",
      "('Accuracy_train:', 0.9995892)\n",
      "('Accuracy_test:', 0.8220697)\n",
      "Generation: 1000. Loss = 78798340.0\n",
      "('Accuracy_train:', 0.99900234)\n",
      "('Accuracy_test:', 0.8189018)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = len(X_train)\n",
    "\n",
    "# loss_vec = []\n",
    "# test_loss = []\n",
    "for i in range(1000):\n",
    "    rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "    rand_x = X_train[rand_index]\n",
    "    rand_y = y_train[rand_index]\n",
    "    sess.run(optimizer, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    test_temp_loss = sess.run(loss, feed_dict={x_data: X_test, y_target: y_test})\n",
    "    test_loss.append(test_temp_loss)\n",
    "    if (i+1)%50==0:\n",
    "        print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(layer_last, 1), tf.argmax(y_target, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print(\"Accuracy_train:\", sess.run(accuracy, feed_dict = {x_data: X_train, y_target: y_train}) )\n",
    "        print(\"Accuracy_test:\", sess.run(accuracy, feed_dict = {x_data: X_test, y_target: y_test}))\n",
    "        \n",
    "#         global result \n",
    "#         result = tf.argmax(pred, 1).eval({x: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
